{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cd59531c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Bert \n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast, AdamW\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "# specify GPU\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c413e2cc",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66a4b3cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  Go until jurong point, crazy.. Available only ...\n",
       "1      0                      Ok lar... Joking wif u oni...\n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      0  U dun say so early hor... U c already then say...\n",
       "4      0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../dataset/spamdata_v2.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af6b0fd",
   "metadata": {},
   "source": [
    "We can see the dataset contains the label, and the Tex column contains the message feature.\n",
    "Lets check the shape next..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e49cc94c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70aefa71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.865937\n",
       "1    0.134063\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check class distribution\n",
    "data['label'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce41921a",
   "metadata": {},
   "source": [
    "### Train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8f20ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, temp_text, train_labels, temp_labels = train_test_split(data['text'], data['label'], \n",
    "                                                                    random_state=420, \n",
    "                                                                    test_size=0.3, \n",
    "                                                                    stratify=data['label'])\n",
    "\n",
    "# we will use temp_text and temp_labels to create validation and test set\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
    "                                                                random_state=420, \n",
    "                                                                test_size=0.5, \n",
    "                                                                stratify=temp_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7236a78a",
   "metadata": {},
   "source": [
    "### Initialize BERT model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffd8148c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ad193b9cb940c98a38331d40738d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e2d5dda7d78421aafd94bdf4284beb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c0c90608c9144c4931ed849a0b3609a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e48260d2ca22490e957c12bcdc3c5ad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d05f46fd2c34c5382d3d69c653cd0ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import BERT-base pretrained model\n",
    "bert_model_base = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0974a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1996, 14324, 2944, 102, 0, 0, 0], [101, 2742, 2951, 2005, 14324, 2944, 17372, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample data\n",
    "text = [\"the bert model\", \"example data for BERT model tuning\"]\n",
    "\n",
    "# encode text\n",
    "sent_id = bert_tokenizer.batch_encode_plus(text, padding=True, return_token_type_ids=False)\n",
    "\n",
    "sent_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a19725",
   "metadata": {},
   "source": [
    "### Using the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4940f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get length of all the messages in the train set\n",
    "seq_len = [len(i.split()) for i in train_text]\n",
    "\n",
    "pd.Series(seq_len).hist(bins = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799a821d",
   "metadata": {},
   "source": [
    "We have to tokenize and encode the entries in the three sets we have created, The training, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f93ee203",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training Set\n",
    "tokens_train = bert_tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    max_length = 25,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# Validation set\n",
    "tokens_val = bert_tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    max_length = 25,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# Test set\n",
    "tokens_test = bert_tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length = 25,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11453a56",
   "metadata": {},
   "source": [
    "Convert squences to Tensors, once again for all three the sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b10509bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "# Validation set\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "# Test set\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d90b4c",
   "metadata": {},
   "source": [
    "Create dataloaders to combine the dataset and sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6048c727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n",
    "\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981297a2",
   "metadata": {},
   "source": [
    "Freeze parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "084c6bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in bert_model_base.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27523708",
   "metadata": {},
   "source": [
    "### Initialize the model, build architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eaf1e4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "      super(BERT_Arch, self).__init__()\n",
    "\n",
    "      self.bert = bert \n",
    "      \n",
    "      self.dropout = nn.Dropout(0.1) # dropout layer\n",
    "    \n",
    "      self.relu =  nn.ReLU() # relu activation function\n",
    "\n",
    "      self.fc1 = nn.Linear(768,512) # dense layer 1\n",
    "      \n",
    "\n",
    "      self.fc2 = nn.Linear(512,2) # dense layer 2 (Output layer)\n",
    "      self.softmax = nn.LogSoftmax(dim=1) # softmax activation function\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):  \n",
    "      _, cls_hs = self.bert(sent_id, attention_mask=mask) #pass the inputs to the model\n",
    "      \n",
    "      x = self.fc1(cls_hs)\n",
    "\n",
    "      x = self.relu(x)\n",
    "\n",
    "      x = self.dropout(x) \n",
    "\n",
    "      x = self.fc2(x) # output layer\n",
    "      x = self.softmax(x) # apply softmax activation\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "04abf44e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m BERT_Arch(bert_model_base)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# push the model to GPU\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:612\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 612\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:359\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 359\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    362\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    363\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    364\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    370\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:359\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 359\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    362\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    363\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    364\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    370\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:359\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 359\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    362\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    363\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    364\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    370\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:381\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m param \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 381\u001b[0m         param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:610\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 610\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/cuda/__init__.py:166\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m msg)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# Pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bert_model_base)\n",
    "\n",
    "# Push the model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr = 1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "83907634",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/basdaniel3/miniconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38807864",
   "metadata": {},
   "source": [
    "Calculate the tensors from class weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "387b95bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.57743559, 3.72848948])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compute the class weights\n",
    "class_wts = compute_class_weight(\n",
    "                                        class_weight = \"balanced\",\n",
    "                                        classes = np.unique(train_labels),\n",
    "                                        y = train_labels                                                    \n",
    "                                    )\n",
    "class_wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72559855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class weights to tensor\n",
    "weights = torch.tensor(class_wts, dtype = torch.float)\n",
    "weights = weights.to(device)\n",
    "\n",
    "# loss function\n",
    "cross_entropy = nn.NLLLoss(weight = weights) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea4172b",
   "metadata": {},
   "source": [
    "We are going to define functions for the training and evaluations of the models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd423d2",
   "metadata": {},
   "source": [
    "Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c2cd2529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train():\n",
    "  model.train()\n",
    "\n",
    "  total_loss, total_accuracy = 0, 0\n",
    "  # list to save model predictions\n",
    "  total_preds=[]\n",
    "  \n",
    "  # iterate over batches\n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "    \n",
    "    # progress update after every 50 batches.\n",
    "    if step % 50 == 0 and not step == 0:\n",
    "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "    # push the batch to gpu\n",
    "    batch = [r.to(device) for r in batch]\n",
    " \n",
    "    sent_id, mask, labels = batch\n",
    "\n",
    "    # clear previously calculated gradients \n",
    "    model.zero_grad()        \n",
    "\n",
    "    # get model predictions for the current batch\n",
    "    preds = model(sent_id, mask)\n",
    "\n",
    "    # compute the loss between actual and predicted values\n",
    "    loss = cross_entropy(preds, labels)\n",
    "\n",
    "    # add on to the total loss\n",
    "    total_loss = total_loss + loss.item()\n",
    "\n",
    "    # backward pass to calculate the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # model predictions are stored on GPU. So, push it to CPU\n",
    "    preds=preds.detach().cpu().numpy()\n",
    "\n",
    "    # append the model predictions\n",
    "    total_preds.append(preds)\n",
    "\n",
    "  # compute the training loss of the epoch\n",
    "  avg_loss = total_loss / len(train_dataloader)\n",
    "  \n",
    "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "  # reshape the predictions in form of (number of samples, no. of classes)\n",
    "  total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "  #returns the loss and predictions\n",
    "  return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92437763",
   "metadata": {},
   "source": [
    "Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "addaa721",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "  print(\"\\nEvaluating\\n\")\n",
    "\n",
    "  # deactivate dropout layers\n",
    "  model.eval()\n",
    "\n",
    "  total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  # empty list to save the model predictions\n",
    "  total_preds = []\n",
    "\n",
    "  # iterate over batches\n",
    "  for step,batch in enumerate(val_dataloader):\n",
    "    \n",
    "    # Progress update every 50 batches.\n",
    "    if step % 50 == 0 and not step == 0:\n",
    "      \n",
    "      # Calculate elapsed time in minutes.\n",
    "      elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "      # Report progress.\n",
    "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "    # push the batch to gpu\n",
    "    batch = [t.to(device) for t in batch]\n",
    "\n",
    "    sent_id, mask, labels = batch\n",
    "\n",
    "    # deactivate autograd\n",
    "    with torch.no_grad():\n",
    "      \n",
    "      # model predictions\n",
    "      preds = model(sent_id, mask)\n",
    "\n",
    "      # compute the validation loss between actual and predicted values\n",
    "      loss = cross_entropy(preds,labels)\n",
    "\n",
    "      total_loss = total_loss + loss.item()\n",
    "\n",
    "      preds = preds.detach().cpu().numpy()\n",
    "\n",
    "      total_preds.append(preds)\n",
    "\n",
    "  # compute the validation loss of the epoch\n",
    "  avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "  # reshape the predictions in form of (number of samples, no. of classes)\n",
    "  total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "  return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4fed53",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e222c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _ = train()\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, _ = evaluate()\n",
    "    \n",
    "    #save the best model we can load in the next step\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254a595f",
   "metadata": {},
   "source": [
    "### Load the saved weights of the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4670dfca",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'saved_weights.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaved_weights.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/serialization.py:581\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    579\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 581\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    583\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    584\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    585\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    586\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 230\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'saved_weights.pt'"
     ]
    }
   ],
   "source": [
    "path = 'saved_weights.pt'\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9ab3a81a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [44]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# get predictions for test data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 3\u001b[0m   preds \u001b[38;5;241m=\u001b[39m model(\u001b[43mtest_seq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, test_mask\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m      4\u001b[0m   preds \u001b[38;5;241m=\u001b[39m preds\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/cuda/__init__.py:166\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m msg)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# get predictions for test data\n",
    "with torch.no_grad():\n",
    "  preds = model(test_seq.to(device), test_mask.to(device))\n",
    "  preds = preds.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "33f83676",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [52]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# model's performance\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[43mpreds\u001b[49m, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(test_y, preds))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preds' is not defined"
     ]
    }
   ],
   "source": [
    "# model's performance\n",
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fa4867d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [51]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# confusion matrix\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m cm \u001b[38;5;241m=\u001b[39m confusion_matrix(test_y, \u001b[43mpreds\u001b[49m)\n\u001b[1;32m      4\u001b[0m disp \u001b[38;5;241m=\u001b[39m ConfusionMatrixDisplay(confusion_matrix\u001b[38;5;241m=\u001b[39mcm)\n\u001b[1;32m      6\u001b[0m disp\u001b[38;5;241m.\u001b[39mplot()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preds' is not defined"
     ]
    }
   ],
   "source": [
    "# confusion matrix\n",
    "cm = confusion_matrix(test_y, preds)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "\n",
    "disp.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1d7e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
